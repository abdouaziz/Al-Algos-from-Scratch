{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "Decision Trees are ML algorithms that progressively divide data sets into smaller data groups based on a descriptive feature, until they reach sets that are small enough to be described by some label.\n",
    "\n",
    "### Main DT Algorithms\n",
    "####  1. CHAID - Chi-squared Automatic Interaction Detection\n",
    "When building **classification trees**, CHAID relies on chi-squared tests to find the best split at each step. In other words, it chooses the independent variable that has the strongest interaction with the dependent variable. For **regression trees**, CHAID relies on F-tests to calculate the difference between two population means.\n",
    "\n",
    "#### 2. CART - Classification And Regression Trees\n",
    "In the case of **Classification Trees**, CART algorithm uses a metric called **Gini Impurity** to create decision points for classification tasks. Gini Impurity gives an idea of how fine a split is. In the case of **Regression Trees**, CART algorithm looks for splits that minimize the **Least Square Deviation (LSD)**, choosing the partitions that minimize the result over all possible options. The LSD (sometimes referred as “variance reduction”) metric minimizes the sum of the squared distances (or deviations) between the observed values and the predicted values.\n",
    "\n",
    "#### 3. ID3 - Iterative Dichotomiser 3\n",
    "It is mostly used for classification tasks. ID3 splits data attributes (dichotomizes) to find the most dominant features, performing this process iteratively to select the DT nodes in a top-down approach. For the splitting process, ID3 uses the **Information Gain** metric to select the most useful attributes for classification. Information Gain is directly linked to the concept of **Entropy**, which is the measure of the amount of uncertainty or randomness in the data.\n",
    "\n",
    "#### 4. C 4.5 \n",
    "It is successor of ID3. C4.5 can handle both continuous and categorical data, making it suitable to generate Regression and Classification Trees. Additionally, it can deal with missing values by ignoring instances that include non-existing data. Unlike ID3, C4.5 uses **Gain Ratio** for its splitting process. Gain Ratio is a modification of the Information Gain concept that reduces the bias on DTs. Another capability of C4.5 is that it can prune DTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will implement CART algorithm for Regression task\n",
    "#### Important points\n",
    "1. The representation of the CART model is a binary tree.\n",
    "2. For regression, The cost function that is minimized to choose split points is the sum squared error(MSE) across all training samples that fall within the node.\n",
    "3. For classification, The Gini cost function is used which provides an indication of how pure the nodes are, where node purity refers to how mixed the training data assigned to each node is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MSE Criterion\n",
    "* Name of the cost function used to evaluate splits in the dataset.\n",
    "* Performs only binary splits\n",
    "* Higher the value of mse higher the error.\n",
    "\n",
    "**Steps to calculate cost**:\n",
    "1. Calculate Mean squared Error of y for each node.\n",
    "2. Calculate Gini for split using weighted mse of each node of that split\n",
    "\n",
    "\n",
    "$$ Cost\\;for\\;split = \\sum ( MSE\\;for\\;node\\;y\\;values * \\frac{node\\;size}{total\\;size}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Terminal Node\n",
    "When to decide to stop growing the tree\n",
    "\n",
    "**Maximum Tree Depth** : This is the maximum number of nodes from the root node of the tree. Once a maximum depth of the tree is met, we must stop splitting adding new nodes. Deeper trees are more complex and are more likely to overfit the training data.\n",
    "\n",
    "**Minimum Node Records** :  This is the minimum number of training patterns that a given node is responsible for. Once at or below this minimum, we must stop splitting and adding new nodes. Nodes that account for too few training patterns are expected to be too specific and are likely to overfit the training data.\n",
    "\n",
    "There is one more condition. It is possible to choose a split in which all rows belong to one group. In this case, we will be unable to continue splitting and adding child nodes as we will have no records to split on one side or another.\n",
    "\n",
    "Now we have some ideas of when to stop growing the tree. When we do stop growing at a given point, that node is called a terminal node and is used to make a final prediction.\n",
    "\n",
    "This is done by taking the group of rows assigned to that node and selecting the most common class value in the group. This will be used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor():\n",
    "    def __init__(self,criterion='mse'):\n",
    "        \"\"\"\n",
    "        Args :\n",
    "        criterion : mse : mean squared error, mae : mean absoulute error, std : standard deviation\n",
    "        \"\"\"\n",
    "        self.root = None\n",
    "        self.max_depth = 0\n",
    "        self.cost = { 'mse':self.__mse,'std':self.__std,'mae':self.__mae }[criterion]\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "    def __std(self,y):\n",
    "        squared_error = (y-y.mean())**2\n",
    "        return np.sqrt(  np.sum(squared_error)/len(y)  )\n",
    "    \n",
    "    def __mse(self,y):\n",
    "        squared_error = (y-y.mean())**2\n",
    "        return np.sum( squared_error/len(y) )\n",
    "    \n",
    "    def __mae(self,y): return np.sum(abs(y-y.mean())/len(y))\n",
    "    \n",
    "    def computeCost(self,groups, y):\n",
    "        n_instances = len(groups[0])+len(groups[1])  # count of all samples\n",
    "        weighted_cost = 0.0 # sum weighted Gini index for each group\n",
    "        for indexes in groups:\n",
    "            size = len(indexes)\n",
    "            # avoid divide by zero\n",
    "            if size == 0: continue\n",
    "            weighted_cost +=  self.cost(y[indexes]) * (size/n_instances)\n",
    "        return weighted_cost\n",
    "    \n",
    "    def get_split(self,X,y):\n",
    "        b_index, b_value, b_cost, b_groups = float('inf'), float('inf'), float('inf'), None\n",
    "        for col_ind in range(X.shape[1]): #no of features\n",
    "            for val in np.unique(X[:,col_ind]): #for each unique value in each of the features\n",
    "\n",
    "                #left_index indexes lower than val for feature, right_index indexes greater that val for feature\n",
    "                left_index = np.reshape( np.argwhere(X[:,col_ind]<val), (-1,) )\n",
    "                right_index = np.reshape( np.argwhere(X[:,col_ind]>=val), (-1,) )\n",
    "                \n",
    "                #find gini index\n",
    "                cost = self.computeCost((left_index,right_index), y)\n",
    "                if cost < b_cost:\n",
    "                    b_index, b_value, b_cost, b_groups = col_ind, val, cost, (left_index, right_index)\n",
    "        return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "    \n",
    "    def to_terminal(self,y): return y.mean()\n",
    "    \n",
    "    def split(self,node, X, y, max_depth, min_samples_split, depth):\n",
    "        self.max_depth = max(depth,self.max_depth)\n",
    "        left, right = node.pop('groups')\n",
    "        \n",
    "        # check for a no split\n",
    "        if len(left)==0 or len(right)==0:\n",
    "            node['left'] = node['right'] = self.to_terminal(y[np.append(left,right)])\n",
    "            return\n",
    "        \n",
    "        # check for max depth\n",
    "        if depth >= max_depth:\n",
    "            node['left'], node['right'] = self.to_terminal(y[left]), self.to_terminal(y[right])\n",
    "            return\n",
    "        \n",
    "        # process left child\n",
    "        if len(left) <= min_samples_split:\n",
    "            node['left'] = self.to_terminal(y[left])\n",
    "        else:\n",
    "            node['left'] = self.get_split(X[left],y[left])\n",
    "            self.split(node['left'], X[left], y[left], max_depth, min_samples_split, depth+1)\n",
    "        \n",
    "        # process right child\n",
    "        if len(right) <= min_samples_split:\n",
    "            node['right'] = self.to_terminal(y[right])\n",
    "        else:\n",
    "            node['right'] = self.get_split(X[right],y[right])\n",
    "            self.split(node['right'],X[right],y[right], max_depth, min_samples_split, depth+1)\n",
    "\n",
    "    def fit(self, X, y, max_depth=None, min_samples_split=2):\n",
    "        self.X, self.y, max_depth = X, y, float('inf') if max_depth==None else max_depth\n",
    "        self.root = self.get_split(X,y)\n",
    "        self.split(self.root, X, y, max_depth, min_samples_split,1)\n",
    "        \n",
    "    def predict_row(self,row,node):\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict): return self.predict_row(row,node['left'])\n",
    "            else: return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict): return self.predict_row(row,node['right'])\n",
    "            else: return node['right']\n",
    "    \n",
    "    def predict(self,rows): return np.array( [self.predict_row(row,self.root) for row in rows] )\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        \"Coefficient of Determination: r2\"\n",
    "        y_pred = self.predict(X)\n",
    "        return 1-( np.sum( (y-y_pred)**2 )/np.sum( (y-y.mean())**2 ) )\n",
    "    \n",
    "    @property\n",
    "    def depth(self): return self.max_depth\n",
    "    \n",
    "    @property\n",
    "    def tree_(self): return self.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.04590</td>\n",
       "      <td>52.5</td>\n",
       "      <td>5.32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405</td>\n",
       "      <td>6.315</td>\n",
       "      <td>45.6</td>\n",
       "      <td>7.3172</td>\n",
       "      <td>6</td>\n",
       "      <td>293</td>\n",
       "      <td>16.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.60</td>\n",
       "      <td>22.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.25356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0.544</td>\n",
       "      <td>5.705</td>\n",
       "      <td>77.7</td>\n",
       "      <td>3.9450</td>\n",
       "      <td>4</td>\n",
       "      <td>304</td>\n",
       "      <td>18.4</td>\n",
       "      <td>396.42</td>\n",
       "      <td>11.50</td>\n",
       "      <td>16.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.19073</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0</td>\n",
       "      <td>0.431</td>\n",
       "      <td>6.718</td>\n",
       "      <td>17.5</td>\n",
       "      <td>7.8265</td>\n",
       "      <td>7</td>\n",
       "      <td>330</td>\n",
       "      <td>19.1</td>\n",
       "      <td>393.74</td>\n",
       "      <td>6.56</td>\n",
       "      <td>26.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.04932</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.472</td>\n",
       "      <td>6.849</td>\n",
       "      <td>70.3</td>\n",
       "      <td>3.1827</td>\n",
       "      <td>7</td>\n",
       "      <td>222</td>\n",
       "      <td>18.4</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.53</td>\n",
       "      <td>28.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>13.07510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>5.713</td>\n",
       "      <td>56.7</td>\n",
       "      <td>2.8237</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.76</td>\n",
       "      <td>20.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
       "288   0.04590  52.5   5.32     0  0.405  6.315  45.6  7.3172    6  293   \n",
       "315   0.25356   0.0   9.90     0  0.544  5.705  77.7  3.9450    4  304   \n",
       "249   0.19073  22.0   5.86     0  0.431  6.718  17.5  7.8265    7  330   \n",
       "307   0.04932  33.0   2.18     0  0.472  6.849  70.3  3.1827    7  222   \n",
       "469  13.07510   0.0  18.10     0  0.580  5.713  56.7  2.8237   24  666   \n",
       "\n",
       "     ptratio       b  lstat  medv  \n",
       "288     16.6  396.90   7.60  22.3  \n",
       "315     18.4  396.42  11.50  16.2  \n",
       "249     19.1  393.74   6.56  26.2  \n",
       "307     18.4  396.90   7.53  28.2  \n",
       "469     20.2  396.90  14.76  20.1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data = data.sample(frac=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((364, 13), (142, 13), (364,), (142,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_test_split(X,y,test_size=0.3):\n",
    "    indexes = np.random.choice( [False,True], len(y), p=[test_size,1-test_size])\n",
    "    return X[indexes],X[~indexes],y[indexes],y[~indexes]\n",
    "\n",
    "X = data.drop('medv',axis=1).values\n",
    "y = data.medv.values\n",
    "\n",
    "X_train,X_val,Y_train,Y_val = train_test_split(X,y)\n",
    "X_train.shape,X_val.shape,Y_train.shape,Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7425657291403263, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(criterion='mse')\n",
    "dt.fit(X_train,Y_train)\n",
    "dt.score(X_val,Y_val),dt.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7.85,  8.3 ,  8.45,  8.8 ,  9.7 , 10.35, 10.5 , 11.4 , 12.3 ,\n",
       "        12.9 , 13.3 , 13.35, 13.4 , 13.6 , 13.95, 14.45, 14.5 , 14.55,\n",
       "        14.85, 15.6 , 16.05, 16.8 , 17.1 , 17.25, 17.4 , 17.55, 18.25,\n",
       "        18.3 , 18.4 , 18.5 , 18.65, 18.7 , 19.  , 19.1 , 19.2 , 19.2 ,\n",
       "        19.25, 19.3 , 19.4 , 19.45, 19.8 , 19.85, 19.9 , 20.1 , 21.2 ,\n",
       "        21.5 , 21.7 , 22.  , 22.1 , 22.15, 22.2 , 22.3 , 22.45, 22.95,\n",
       "        23.15, 23.6 , 23.9 , 24.1 , 24.15, 24.35, 24.4 , 24.6 , 24.9 ,\n",
       "        25.  , 25.3 , 27.9 , 28.  , 28.6 , 28.85, 29.6 , 29.75, 29.95,\n",
       "        30.85, 31.55, 32.25, 32.7 , 32.95, 35.  , 36.2 , 36.3 , 37.2 ,\n",
       "        37.6 , 39.25, 43.3 , 43.9 , 47.5 , 50.  ]),\n",
       " array([2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1,\n",
       "        3, 1, 1, 1, 1, 2, 2, 5, 2, 1, 2, 2, 4, 1, 4, 1, 1, 1, 1, 2, 1, 1,\n",
       "        2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1,\n",
       "        2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 3, 3, 1, 1, 2, 1, 1, 2, 1, 8]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dt.predict(X_val),return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
