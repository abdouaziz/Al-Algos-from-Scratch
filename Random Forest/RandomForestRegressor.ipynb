{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from decisiontree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForestRegressor():\n",
    "    \"\"\"\n",
    "    Random Forest fits number of decision tree on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "    The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    n_estimators : integer (Default 50), number of trees in forest\n",
    "    \n",
    "    criterion : ('mse', 'mae', 'std') ( Default 'mse' )\n",
    "        The function to measure the quality of a split.\n",
    "        'mse' is mean squared error\n",
    "        'mae' is mean absolute error\n",
    "        'std' is standard deviation\n",
    "        \n",
    "    max_depth : integer (Default 'inf'), maximum depth allowed for each tree\n",
    "    \n",
    "    min_samples_split : integer (Default 2), The minimum number of samples required to split an internal node\n",
    "   \n",
    "    max_features : ( 'auto', 'sqrt', 'log2', 'max_features' ) ( Default 'auto' )\n",
    "        The number of features to consider when looking for the best split:\n",
    "        'auto' is same as 'sqrt'\n",
    "        'sqrt' is sqrt(number of features)\n",
    "        'log2' is log2(number of features)\n",
    "        'max_features' is all features\n",
    "    \n",
    "    bootstrap : If False, the whole datset is used to build each tree.\n",
    "   \n",
    "    random_state : random seed\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,  n_estimators=10, criterion='mse', max_depth = None, min_samples_split=2, max_features=\"auto\", \n",
    "                 bootstrap=True, random_state=None):\n",
    "        np.random.seed( random_state if random_state!=None else np.random.randint(100)  )\n",
    "        self.__n_estimators = n_estimators\n",
    "        self.__criterion = criterion\n",
    "        self.__max_depth = float('inf') if max_depth==None else max_depth\n",
    "        self.__min_samples_split = min_samples_split\n",
    "        self.__max_features = { \n",
    "            'auto': lambda x: int(np.sqrt(x))+1, 'sqrt': lambda x: int(np.sqrt(x))+1, \n",
    "            'log2': lambda x: int(np.log2(x))+1, 'max_features': lambda x: x  }[max_features]\n",
    "        self.__bootstrap = bootstrap\n",
    "        self.__n_samples = None\n",
    "        self.__n_features = None\n",
    "        self.__trees = [  ]\n",
    "        \n",
    "    def __bootstrapX(self,X):\n",
    "        indexes = np.random.choice( np.arange(0,len(X),1), size=self.__n_samples, replace=self.__bootstrap )\n",
    "        return X[indexes,:]\n",
    "    \n",
    "    def __get_feature_index(self): \n",
    "        return np.random.choice( np.arange(0,self.__n_features,1), \n",
    "                                size=self.__max_features(self.__n_features), replace=False)\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        Fit the X and y to estimators\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        X : numpy array, independent variables\n",
    "        \n",
    "        y : numpy array, target variable\n",
    "        \n",
    "        \"\"\"\n",
    "        self.__n_samples, self.__n_features = X.shape\n",
    "        X_y = np.c_[X,y]\n",
    "        for _ in range(self.__n_estimators):\n",
    "            dt = DecisionTreeRegressor( criterion=self.__criterion, max_depth=self.__max_depth, \n",
    "                                        min_samples_split=self.__min_samples_split)\n",
    "            data = self.__bootstrapX(X_y)\n",
    "            features = self.__get_feature_index()\n",
    "            dt.fit( data[:,features], data[:,-1] )\n",
    "            self.__trees.append( (dt.tree_, features) )\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Predict values using all estimators\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, independent variables\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predicted values\n",
    "        \n",
    "        \"\"\"\n",
    "        predictions = np.zeros( (len(X)) )\n",
    "        for root, features in self.__trees:\n",
    "            predictions += np.array([ self.__predict_row(row,root) for row in X[:,features] ])\n",
    "        return predictions/self.__n_estimators\n",
    "            \n",
    "    def __predict_row(self,row,node):\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict): return self.__predict_row(row,node['left'])\n",
    "            else: return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict): return self.__predict_row(row,node['right'])\n",
    "            else: return node['right']\n",
    "            \n",
    "    def score(self,X,y):\n",
    "        \"\"\"\n",
    "        Computer Coefficient of Determination (rsquare)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D numpy array, independent variables\n",
    "        \n",
    "        y : numpy array, dependent variables\n",
    "       \n",
    "        Returns\n",
    "        -------\n",
    "        r2 values\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return 1-( np.sum( (y-y_pred)**2 )/np.sum( (y-y.mean())**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data.csv',).values\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_t,X_v,Y_t,Y_v = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6993943575049519\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=100,criterion='mse',\n",
    "                            max_depth=8,max_features='auto')\n",
    "rfr.fit(X_t,Y_t)\n",
    "print(rfr.score(X_v,Y_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
