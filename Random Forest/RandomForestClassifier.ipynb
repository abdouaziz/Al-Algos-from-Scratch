{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from decisiontree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForestClassifier():\n",
    "    \"\"\"\n",
    "    Random Forest fits number of decision tree on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "    The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    n_estimators : integer (Default 50), number of trees in forest\n",
    "    \n",
    "    max_depth : integer (Default 'inf'), maximum depth allowed for each tree\n",
    "    \n",
    "    min_samples_split : integer (Default 2), The minimum number of samples required to split an internal node\n",
    "    \n",
    "    max_features : ( 'auto', 'sqrt', 'log2', 'max_features' ) \n",
    "        The number of features to consider when looking for the best split:\n",
    "        'auto' is same as 'sqrt'\n",
    "        'sqrt' is sqrt(number of features)\n",
    "        'log2' is log2(number of features)\n",
    "        'max_features' is all features\n",
    "    \n",
    "    bootstrap : If False, the whole datset is used to build each tree.\n",
    "    \n",
    "    random_state : random seed\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=50, max_depth = None, min_samples_split=2, max_features=\"auto\", \n",
    "                 bootstrap=True, random_state=None):\n",
    "        np.random.seed( random_state if random_state!=None else np.random.randint(100)  )\n",
    "        self.__n_estimators = n_estimators\n",
    "        self.__max_depth = float('inf') if max_depth==None else max_depth\n",
    "        self.__min_samples_split = min_samples_split\n",
    "        self.__max_features = { \n",
    "            'auto': lambda x: int(np.sqrt(x)), 'sqrt': lambda x: int(np.sqrt(x)), \n",
    "            'log2': lambda x: int(np.log2(x)), 'max_features': lambda x: x  }[max_features]\n",
    "        self.__bootstrap = bootstrap\n",
    "        self.__n_samples = None\n",
    "        self.__n_features = None\n",
    "        self.__n_classes = None\n",
    "        self.__trees = [  ]\n",
    "        \n",
    "    def __bootstrapX(self,X):\n",
    "        indexes = np.random.choice( np.arange(0,len(X),1), size=self.__n_samples, replace=self.__bootstrap )\n",
    "        return X[indexes,:]\n",
    "    \n",
    "    def __get_feature_index(self): \n",
    "        return np.random.choice( np.arange(0,self.__n_features,1), \n",
    "                                size=self.__max_features(self.__n_features), replace=False)\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        Fit the X and y to estimators\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, independent variables\n",
    "        \n",
    "        y : numpy array, target variable\n",
    "        \n",
    "        \"\"\"\n",
    "        self.__n_samples, self.__n_features = X.shape\n",
    "        self.__n_classes = len(np.unique(y))\n",
    "        X_y = np.c_[X,y]\n",
    "        for _ in range(self.__n_estimators):\n",
    "            dt = DecisionTreeClassifier( max_depth=self.__max_depth, \n",
    "                                        min_samples_split=self.__min_samples_split, \n",
    "                                       n_classes = self.__n_classes)\n",
    "            data = self.__bootstrapX(X_y)\n",
    "            features = self.__get_feature_index()\n",
    "            dt.fit( data[:,features], data[:,-1] )\n",
    "            self.__trees.append( (dt.tree_, features) )\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Predict labels using all estimators\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, independent variables\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predicted labels\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.argmax( self.predict_proba(X), axis=1 )\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        \"\"\"\n",
    "        Predict probaibilty of each class using all estimators\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, independent variablesss\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        probability of each class [ n_samples, n_classes ] \n",
    "        \n",
    "        \"\"\"\n",
    "        probs = np.zeros( (len(X),self.__n_classes) )\n",
    "        for root, features in self.__trees:\n",
    "            probs += np.array([ self.__predict_row(row,root)[1] for row in X[:,features] ])\n",
    "        return probs/self.__n_estimators\n",
    "        \n",
    "    def __predict_row(self,row,node):\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict):  return self.__predict_row(row,node['left'])\n",
    "            else: return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict): return self.__predict_row(row,node['right'])\n",
    "            else: return node['right']\n",
    "            \n",
    "    def score(self,X,y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy from independent variables\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, independent variables\n",
    "        \n",
    "        y : numpy array, dependent variable\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        accuracy score\n",
    "        \n",
    "        \"\"\"\n",
    "        return (y==self.predict(X)).sum()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1372, 4), (1372,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data_banknote.txt', header=None).values\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_t,X_v,Y_t,Y_v = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.941747572815534"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc  = RandomForestClassifier(n_estimators=100, max_depth=3,min_samples_split=2)\n",
    "rfc.fit(X_t,Y_t)\n",
    "rfc.score(X_v,Y_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
