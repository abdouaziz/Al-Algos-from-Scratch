{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from decisiontree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classification\n",
    "\n",
    "> Regression and classification\n",
    "  only differ in the concrete ``LossFunction`` used.\n",
    "\n",
    "> GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage ``n_classes_`` regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. \n",
    "\n",
    "> Binary classification is a special case where only a single regression tree is induced.\n",
    "\n",
    "> Classification with more than 2 classes requires the induction of n_classes regression trees at each iteration, thus the total number of induced trees equals n_classes * n_estimators.\n",
    "\n",
    "> **Below are the instructions for binary classification**\n",
    "\n",
    "> Initialize the model with a constant probability: \n",
    "\n",
    "> $ 1. \\; \\;  F_0(x) =  max \\frac{count(y=i)}{count(y))} \\;\\;\\; \\text{ where i=0,1 }$\n",
    "\n",
    "> For m = 1 to M,\n",
    "\n",
    "> $ 2. \\; \\; \\text{Compute Psuedo Residuals, } r_{im} = - [ \\frac{ \\partial Loss( y_i , F(x_i) ) }{ \\partial F(x_i) } ]_{F(x) = F_{m-1}(x)} \\;\\;\\; \\text{for i = 1,2...n} $\n",
    "\n",
    ">    $ \\; \\; \\; \\;\\text{For binomial deviance, psuedo residuals } r_{i} =  -1 * \\frac{ \\partial \\big( -y_ilog(p_i) - (1-y_i)log(1-p_i) \\big)}{\\partial p_i} = -1 * -(y_i - p_i)  = y_i - p_i $ \n",
    "\n",
    "> $ 3. \\; \\; \\text{Fit base trainer, } h_m(x) \\text{ to psuedo residuals} $\n",
    "\n",
    "> $ 4. \\; \\; \\text{Update } F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x) $\n",
    "\n",
    "> $ \\; \\; \\; \\; \\; \\gamma_m \\text{ step-size is choosen using line search, } \\gamma_m = arg\\;min_\\gamma \\sum_{i=1}^n L(y_i,F_{m-1}(x_i)) - \\gamma \\frac{\\partial L(y_i,F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)}$\n",
    "\n",
    "> $$ \\; \\; \\text{Final model will be, } F(x) = \\frac{1}{n} \\sum_{i=1}^n y_i + \\sum_{m=1}^M \\gamma_m h_m(x) $$\n",
    "\n",
    "\n",
    "* In code below, step size is constant.\n",
    "* [Nice Article](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)\n",
    "* [Nice Article](http://zpz.github.io/blog/gradient-boosting-tree-for-binary-classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingClassifier():\n",
    "    def __init__(self, loss='deviance', learning_rate = 0.1, n_estimators=100, criterion='mse', \n",
    "                 max_depth=None, min_samples_split=2, max_features=None, verbose=False):\n",
    "        self.__lr = learning_rate\n",
    "        self.__n_estimators = n_estimators\n",
    "        self.__criterion = criterion\n",
    "        self.__max_depth = max_depth\n",
    "        self.__min_samples_split = min_samples_split\n",
    "        self.__max_features = None\n",
    "        if isinstance(max_features,str):\n",
    "            self.__max_features = { \n",
    "            'auto': lambda x: int(np.sqrt(x)), 'sqrt': lambda x: int(np.sqrt(x)), \n",
    "            'log2': lambda x: int(np.log2(x)), 'max_features': lambda x: x  }[max_features]\n",
    "        elif isinstance(max_features, int):\n",
    "            self.__max_features = lambda x: max_features\n",
    "        elif isinstance(max_features, float):\n",
    "            self.__max_features = lambda x: int(max_features*x)\n",
    "        else:\n",
    "            self.__max_features = lambda x: x\n",
    "            \n",
    "        self.__n_features = None\n",
    "        self.__trees = []\n",
    "        self.__verbose = verbose\n",
    "        self.__f0 = None\n",
    "    \n",
    "    def __binomial_deviance(self,p_pred,y_true):\n",
    "        return np.sum(-y_true*np.log(p_pred) - (1-y_true)*np.log(1-p_pred))\n",
    "    \n",
    "    def __negative_binomial_deviance_gradient(self,p_pred,y_true):\n",
    "        grad =  -1 * (y_true - p_pred)\n",
    "        return -1 * grad\n",
    "    \n",
    "    def __get_feature_index(self): \n",
    "        return np.random.choice( np.arange(0,self.__n_features,1), \n",
    "                                size=self.__max_features(self.__n_features), replace=False)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.__n_features = X.shape[1]\n",
    "        p = self.__f0 = max( (y==1).sum(), (y==0).sum()) / len(y)\n",
    "        if self.__verbose:\n",
    "            print( f\"Binomial Deviance Loss, Accuracy after trees {0} : {self.__binomial_deviance(p,y)}, {self.score(X,y)}\" )\n",
    "        for i in range(0,self.__n_estimators):\n",
    "            dt = DecisionTreeRegressor(criterion=self.__criterion, \n",
    "                                       max_depth=self.__max_depth, \n",
    "                                       min_samples_split=self.__min_samples_split)\n",
    "            feature_index = self.__get_feature_index()\n",
    "            h = self.__negative_binomial_deviance_gradient(p,y)\n",
    "            dt.fit(X[:,feature_index], h)\n",
    "            self.__trees.append( (dt.tree_,feature_index) )\n",
    "            p = self.predict_proba(X)[:,1]\n",
    "            if self.__verbose and (i+1)%5==0:\n",
    "                print( f\"Binomial Deviance Loss, Accuracy after trees {i+1} : {self.__binomial_deviance(p,y)}, {self.score(X,y)}\" )\n",
    "            \n",
    "    def predict_proba(self,X):    \n",
    "        predictions = np.ones( len(X) ) * self.__f0\n",
    "        for i in range(1,len(self.__trees)+1):\n",
    "            root, features = self.__trees[i-1]\n",
    "            predictions += self.__lr * np.array([ self.__predict_row(row,root) for row in X[:,features] ])\n",
    "        proba = np.zeros( (len(X),2) )\n",
    "        proba[:,0] = (1-predictions)\n",
    "        proba[:,1] = predictions\n",
    "        return proba\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:,1]>0.5)*1\n",
    "            \n",
    "    def __predict_row(self,row,node):\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict): return self.__predict_row(row,node['left'])\n",
    "            else: return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict): return self.__predict_row(row,node['right'])\n",
    "            else: return node['right']\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        y_pred = self.predict(X)\n",
    "        return (y_pred==y).sum()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1372, 4), (1372,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data_banknote.txt',header=None).values\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_t,X_v,Y_t,Y_v = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binomial Deviance Loss, Accuracy after trees 0 : 677.5610432045382, 0.45416666666666666\n",
      "Binomial Deviance Loss, Accuracy after trees 5 : 607.9841443346995, 0.45416666666666666\n",
      "Binomial Deviance Loss, Accuracy after trees 10 : 547.7972720514115, 1.0\n",
      "Binomial Deviance Loss, Accuracy after trees 15 : 495.24761889084925, 1.0\n",
      "Binomial Deviance Loss, Accuracy after trees 20 : 449.0177164886579, 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9781553398058253)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = GradientBoostingClassifier(learning_rate=0.015,\n",
    "                                n_estimators=20,\n",
    "                               verbose=True)\n",
    "gbr.fit(X_t,Y_t)\n",
    "gbr.score(X_t,Y_t),gbr.score(X_v,Y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
