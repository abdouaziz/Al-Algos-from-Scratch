{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from decisiontree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression\n",
    "\n",
    "> GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.\n",
    "\n",
    "> Initialize the model with a constant value: \n",
    "\n",
    "> $ 1. \\; \\;  F_0(x) =  arg \\; min_\\gamma \\sum_{i=1}^n Loss(y_i, \\gamma) = \\frac{1}{n} \\sum_{i=1}^n y_i  $\n",
    "\n",
    "> For m = 1 to M,\n",
    "\n",
    "> $ 2. \\; \\; \\text{Compute Psuedo Residuals, } r_{im} = - [ \\frac{ \\partial Loss( y_i , F(x_i) ) }{ \\partial F(x_i) } ]_{F(x) = F_{m-1}(x)} \\;\\;\\; \\text{for i = 1,2...n} $\n",
    "\n",
    ">    $ \\; \\; \\; \\;\\text{For MSE Loss, psuedo residuals } r_{i} = - \\frac{\\partial(y_i-\\hat y_i)^2}{\\partial \\hat y_i} = - (-2) ( y_i - \\hat y_i ) = 2( y_i - \\hat y_i) $ \n",
    "\n",
    "> $ 3. \\; \\; \\text{Fit base trainer, } h_m(x) \\text{ to psuedo residuals} $\n",
    "\n",
    "> $ 4. \\; \\; \\text{Update } F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x) $\n",
    "\n",
    "> $ \\; \\; \\; \\; \\; \\gamma_m \\text{ step-size is choosen using line search, } \\gamma_m = arg\\;min_\\gamma \\sum_{i=1}^n L(y_i,F_{m-1}(x_i)) - \\gamma \\frac{\\partial L(y_i,F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)}$\n",
    "\n",
    "> $$ \\; \\; \\text{Final model will be, } F(x) = \\frac{1}{n} \\sum_{i=1}^n y_i + \\sum_{m=1}^M \\gamma_m h_m(x) $$\n",
    "\n",
    "\n",
    "* In code below, step size is constant.\n",
    "* [Nice Article](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)\n",
    "* [Nice Article](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/?source=post_page-----1e317ae4587d----------------------)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor():\n",
    "    def __init__(self, loss='ls', learning_rate = 0.1, n_estimators=100, criterion='mse', \n",
    "                 max_depth=None, min_samples_split=2, max_features=None, verbose=False):\n",
    "        self.__lr = learning_rate\n",
    "        self.__n_estimators = n_estimators\n",
    "        self.__criterion = criterion\n",
    "        self.__max_depth = max_depth\n",
    "        self.__min_samples_split = min_samples_split\n",
    "        self.__max_features = None\n",
    "        if isinstance(max_features,str):\n",
    "            self.__max_features = { \n",
    "            'auto': lambda x: int(np.sqrt(x)), 'sqrt': lambda x: int(np.sqrt(x)), \n",
    "            'log2': lambda x: int(np.log2(x)), 'max_features': lambda x: x  }[max_features]\n",
    "        elif isinstance(max_features, int):\n",
    "            self.__max_features = lambda x: max_features\n",
    "        elif isinstance(max_features, float):\n",
    "            self.__max_features = lambda x: int(max_features*x)\n",
    "        else:\n",
    "            self.__max_features = lambda x: x\n",
    "            \n",
    "        self.__n_features = None\n",
    "        self.__trees = []\n",
    "        self.__verbose = verbose\n",
    "        self.__f0 = None\n",
    "    \n",
    "    def __mse(self,y_pred,y_true):\n",
    "        return np.sqrt( np.mean( (y_true-y_pred)**2 ) )\n",
    "    \n",
    "    def __negative_least_squares_gradient(self,y_pred,y_true):\n",
    "        grad =  -2 * (y_true - y_pred)\n",
    "        return -1 * grad\n",
    "    def __get_feature_index(self): \n",
    "        return np.random.choice( np.arange(0,self.__n_features,1), \n",
    "                                size=self.__max_features(self.__n_features), replace=False)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.__n_features = X.shape[1]\n",
    "        y_ = self.__f0 = y.mean()\n",
    "        for i in range(0,self.__n_estimators):\n",
    "            dt = DecisionTreeRegressor(criterion=self.__criterion, \n",
    "                                       max_depth=self.__max_depth, \n",
    "                                       min_samples_split=self.__min_samples_split)\n",
    "            feature_index = self.__get_feature_index()\n",
    "            h = self.__negative_least_squares_gradient(y_,y)\n",
    "            dt.fit(X[:,feature_index], h)\n",
    "            self.__trees.append( (dt.tree_,feature_index) )\n",
    "            y_ = self.predict(X)\n",
    "            if self.__verbose and i%5==0:\n",
    "                print( f\"MSE after trees {i+1} : {self.__mse(y_,y)}\" )\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.ones( len(X) ) * self.__f0\n",
    "        for i in range(1,len(self.__trees)+1):\n",
    "            root, features = self.__trees[i-1]\n",
    "            predictions += self.__lr * np.array([ self.__predict_row(row,root) for row in X[:,features] ])\n",
    "        return predictions\n",
    "            \n",
    "    def __predict_row(self,row,node):\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict): return self.__predict_row(row,node['left'])\n",
    "            else: return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict): return self.__predict_row(row,node['right'])\n",
    "            else: return node['right']\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        y_pred = self.predict(X)\n",
    "        return 1 - np.sum(np.square(y-y_pred))/np.sum(np.square(y-y.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data.csv',).values\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_t,X_v,Y_t,Y_v = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9998620510817293, 0.8385563972264015)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(learning_rate=0.1,\n",
    "                                n_estimators=20,\n",
    "                               verbose=False)\n",
    "gbr.fit(X_t,Y_t)\n",
    "gbr.score(X_t,Y_t),gbr.score(X_v,Y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
