{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "The idea in logistic regression is to cast the problem in form of generalized linear regression model.\n",
    "\n",
    "Odds of the outcome are $ odds(p) = \\frac{p}{1-p} $ but this has a range of (0,inf). We need a range of (-inf,inf).\n",
    "\n",
    "This can be done using natural log: $ logit(p) = log(\\frac{p}{1-p}) $. Now this is linear and continuous.\n",
    "\n",
    "So we can write it as: $ logit(p) = \\hat Y = bX_i + a $\n",
    "\n",
    "$$ \\hat Y = logit(p) = log(\\frac{p}{1-p}) $$\n",
    "\n",
    "$$ e^{ \\hat Y } =  e^{logit(p)} = \\frac{p}{1-p} $$\n",
    "\n",
    "$$ e^{ \\hat Y } + 1 = \\frac{p}{1-p} + 1 = \\frac{1}{1-p} $$\n",
    "\n",
    "$$ 1-p = \\frac{1}{e^{ \\hat Y } + 1} $$\n",
    "\n",
    "$$ p = 1 - \\frac{1}{e^{ \\hat Y } + 1} $$\n",
    "\n",
    "$$ p = \\frac{e^{ \\hat Y }}{e^{ \\hat Y }+1} = sigmoid(\\hat Y)$$\n",
    "\n",
    "We can use the cross-entropy loss function given by:\n",
    "\n",
    "$$ Loss(b,a) = \\sum_{i=1}^n -Y_ilog(p_i) - (1-Y_i)log(1-p_i) $$\n",
    "\n",
    "$$ Loss(b,a) = \\sum_{i=1}^n log(1-p_i) + \\sum_{i=1}^n Y_i log(\\frac{p_i}{1-p_i}) $$\n",
    "\n",
    "$$ Loss(b,a) = \\sum_{i=1}^n -log(1+e^{bX_i + a}) + \\sum_{i=1}^n Y_i (bX_i + a) $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial b_j} = - \\sum_{i=1}^{n} \\frac{1}{1+e^{bX_i+a}}e^{bX_i+a}X_{ij} + \\sum_{i=1}^n Y_iX_{ij} = \\sum_{i=1}^n (Y_i - p_i)X_{ij} $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial a} = - \\sum_{i=1}^{n} \\frac{1}{1+e^{bX_i+a}}e^{bX_i+a} + \\sum_{i=1}^n Y_i = \\sum_{i=1}^n (Y_i - p_i) $$\n",
    "\n",
    "We are not going to be able to set this to zero and solve exactly. There is no closed-form solution for this equation.\n",
    "\n",
    "So why it worked so well for linear regression? Because once you compute your derivatives you will notice, that resulting problem is set of linear equations, m equations with m variables, which we know can be directly solved through matrix inversions (and other techniques). When you differentiate logistic regression cost, resulting problem is no longer linear... it is convex (thus global optimum), but not linear, and consequently - current mathematics does not provide us with tools strong enough to find the optimum in closed form solution.\n",
    "\n",
    "[Nice Article](https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf)\n",
    "[Nice Article](https://aria42.com/blog/2014/12/understanding-lbfgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
